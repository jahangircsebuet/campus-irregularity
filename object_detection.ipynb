{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eeb35fad37be4a218a399dc358d595ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc575c556b3740f8ae2c7085f9ede50a",
              "IPY_MODEL_34da890d24374b88a23a2b933304a303",
              "IPY_MODEL_8af3d6eb9dc3415b8560d85c9e57364f"
            ],
            "layout": "IPY_MODEL_768f641b599849039ffbe39be193b256"
          }
        },
        "bc575c556b3740f8ae2c7085f9ede50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d717f78bb043eaac4318e8053569b0",
            "placeholder": "​",
            "style": "IPY_MODEL_3acf732f05c2424c99cc310ed7afcb21",
            "value": "Loss: 5.7844:  34%"
          }
        },
        "34da890d24374b88a23a2b933304a303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_653bd80e9f8342278e86c461862bd91a",
            "max": 114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aafcf04e05c84a78b1e4f739266a4b91",
            "value": 39
          }
        },
        "8af3d6eb9dc3415b8560d85c9e57364f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f6c894f78d849ed8e330a33583e1504",
            "placeholder": "​",
            "style": "IPY_MODEL_195d06bb5aa14f97af5f7fdb6c3074ea",
            "value": " 39/114 [50:16&lt;1:32:18, 73.85s/it]"
          }
        },
        "768f641b599849039ffbe39be193b256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63d717f78bb043eaac4318e8053569b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3acf732f05c2424c99cc310ed7afcb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "653bd80e9f8342278e86c461862bd91a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aafcf04e05c84a78b1e4f739266a4b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f6c894f78d849ed8e330a33583e1504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "195d06bb5aa14f97af5f7fdb6c3074ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## config.py"
      ],
      "metadata": {
        "id": "21GuZ1pos6kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d16bi_c6tCx1",
        "outputId": "18833f84-7841-4fa7-bfa0-c273cdedf051"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install torchvision\n",
        "# !pip install torchmetrics\n",
        "# !pip install tqdm\n",
        "# !pip install matplotlib"
      ],
      "metadata": {
        "id": "mZFikz4DIyQA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## config"
      ],
      "metadata": {
        "id": "nfL4LO9QLOOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "BATCH_SIZE = 16 # Increase / decrease according to GPU memeory.\n",
        "RESIZE_TO = 640 # Resize the image for training and transforms.\n",
        "NUM_EPOCHS = 100 # Number of epochs to train for.\n",
        "NUM_WORKERS = 2 # Number of parallel workers for data loading.\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Training images and XML files directory.\n",
        "# TRAIN_DIR = 'data/Train/Train/JPEGImages'\n",
        "# TRAIN_DIR = '/content/drive/MyDrive/SIU/Fall23/Courses/ML/Project/PersonDataset/Train'\n",
        "TRAIN_DIR = '/content/drive/MyDrive/SIU/Fall23/Courses/ML/Project/Dataset/Train'\n",
        "# Validation images and XML files directory.\n",
        "# VALID_DIR = 'data/Val/Val/JPEGImages'\n",
        "# VALID_DIR = '/content/drive/MyDrive/SIU/Fall23/Courses/ML/Project/PersonDataset/Val'\n",
        "VALID_DIR = '/content/drive/MyDrive/SIU/Fall23/Courses/ML/Project/Dataset/Val'\n",
        "# Classes: 0 index is reserved for background.\n",
        "CLASSES = [\n",
        "    'cigarette', 'knife', 'Automatic Rifle', 'Bazooka', 'Grenade Launcher', 'Handgun', 'shotgun', 'SMG', 'Sniper', 'Sword'\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "# Whether to visualize images after crearing the data loaders.\n",
        "VISUALIZE_TRANSFORMED_IMAGES = False\n",
        "\n",
        "# Location to save model and plots.\n",
        "OUT_DIR = 'outputs'"
      ],
      "metadata": {
        "id": "-kV8rAZPLP5W"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "Q0jiLUuILd96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "# from config import DEVICE, CLASSES\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# This class keeps track of the training and validation loss values\n",
        "# and helps to get the average for each epoch as well.\n",
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "class SaveBestModel:\n",
        "    \"\"\"\n",
        "    Class to save the best model while training. If the current epoch's\n",
        "    validation mAP @0.5:0.95 IoU higher than the previous highest, then save the\n",
        "    model state.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, best_valid_map=float(0)\n",
        "    ):\n",
        "        self.best_valid_map = best_valid_map\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        model,\n",
        "        current_valid_map,\n",
        "        epoch,\n",
        "        OUT_DIR,\n",
        "    ):\n",
        "        if current_valid_map > self.best_valid_map:\n",
        "            self.best_valid_map = current_valid_map\n",
        "            print(f\"\\nBEST VALIDATION mAP: {self.best_valid_map}\")\n",
        "            print(f\"\\nSAVING BEST MODEL FOR EPOCH: {epoch+1}\\n\")\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                }, f\"{OUT_DIR}/best_model.pth\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    To handle the data loading as different images may have different number\n",
        "    of objects and to handle varying size tensors as well.\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Define the training tranforms.\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Blur(blur_limit=3, p=0.1),\n",
        "        A.MotionBlur(blur_limit=3, p=0.1),\n",
        "        A.MedianBlur(blur_limit=3, p=0.1),\n",
        "        A.ToGray(p=0.3),\n",
        "        A.RandomBrightnessContrast(p=0.3),\n",
        "        A.ColorJitter(p=0.3),\n",
        "        A.RandomGamma(p=0.3),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], bbox_params={\n",
        "        'format': 'pascal_voc',\n",
        "        'label_fields': ['labels']\n",
        "    })\n",
        "\n",
        "# Define the validation transforms.\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], bbox_params={\n",
        "        'format': 'pascal_voc',\n",
        "        'label_fields': ['labels']\n",
        "    })\n",
        "\n",
        "\n",
        "def show_tranformed_image(train_loader):\n",
        "    \"\"\"\n",
        "    This function shows the transformed images from the `train_loader`.\n",
        "    Helps to check whether the tranformed images along with the corresponding\n",
        "    labels are correct or not.\n",
        "    Only runs if `VISUALIZE_TRANSFORMED_IMAGES = True` in config.py.\n",
        "    \"\"\"\n",
        "    if len(train_loader) > 0:\n",
        "        for i in range(1):\n",
        "            images, targets = next(iter(train_loader))\n",
        "            images = list(image.to(DEVICE) for image in images)\n",
        "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
        "            labels = targets[i]['labels'].cpu().numpy().astype(np.int32)\n",
        "            sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "            sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n",
        "            for box_num, box in enumerate(boxes):\n",
        "                cv2.rectangle(sample,\n",
        "                            (box[0], box[1]),\n",
        "                            (box[2], box[3]),\n",
        "                            (0, 0, 255), 2)\n",
        "                cv2.putText(sample, CLASSES[labels[box_num]],\n",
        "                            (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                            1.0, (0, 0, 255), 2)\n",
        "            cv2.imshow('Transformed image', sample)\n",
        "            cv2.waitKey(0)\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "def save_model(epoch, model, optimizer):\n",
        "    \"\"\"\n",
        "    Function to save the trained model till current epoch, or whenver called\n",
        "    \"\"\"\n",
        "    torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                }, 'outputs/last_model.pth')\n",
        "\n",
        "def save_loss_plot(\n",
        "    OUT_DIR,\n",
        "    train_loss_list,\n",
        "    x_label='iterations',\n",
        "    y_label='train loss',\n",
        "    save_name='train_loss'\n",
        "):\n",
        "    \"\"\"\n",
        "    Function to save both train loss graph.\n",
        "\n",
        "    :param OUT_DIR: Path to save the graphs.\n",
        "    :param train_loss_list: List containing the training loss values.\n",
        "    \"\"\"\n",
        "    figure_1 = plt.figure(figsize=(10, 7), num=1, clear=True)\n",
        "    train_ax = figure_1.add_subplot()\n",
        "    train_ax.plot(train_loss_list, color='tab:blue')\n",
        "    train_ax.set_xlabel(x_label)\n",
        "    train_ax.set_ylabel(y_label)\n",
        "    figure_1.savefig(f\"{OUT_DIR}/{save_name}.png\")\n",
        "    print('SAVING PLOTS COMPLETE...')\n",
        "\n",
        "def save_mAP(OUT_DIR, map_05, map):\n",
        "    \"\"\"\n",
        "    Saves the mAP@0.5 and mAP@0.5:0.95 per epoch.\n",
        "    :param OUT_DIR: Path to save the graphs.\n",
        "    :param map_05: List containing mAP values at 0.5 IoU.\n",
        "    :param map: List containing mAP values at 0.5:0.95 IoU.\n",
        "    \"\"\"\n",
        "    figure = plt.figure(figsize=(10, 7), num=1, clear=True)\n",
        "    ax = figure.add_subplot()\n",
        "    ax.plot(\n",
        "        map_05, color='tab:orange', linestyle='-',\n",
        "        label='mAP@0.5'\n",
        "    )\n",
        "    ax.plot(\n",
        "        map, color='tab:red', linestyle='-',\n",
        "        label='mAP@0.5:0.95'\n",
        "    )\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('mAP')\n",
        "    ax.legend()\n",
        "    figure.savefig(f\"{OUT_DIR}/map.png\")"
      ],
      "metadata": {
        "id": "JwzMEsEJLGIc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "i_k-V0ApLm28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob as glob\n",
        "\n",
        "from xml.etree import ElementTree as et\n",
        "# from config import (\n",
        "#     CLASSES, RESIZE_TO, TRAIN_DIR, BATCH_SIZE\n",
        "# )\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from custom_utils import collate_fn, get_train_transform, get_valid_transform\n",
        "\n",
        "# The dataset class.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dir_path, width, height, classes, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.dir_path = dir_path\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.classes = classes\n",
        "        self.image_file_types = ['*.jpg', '*.jpeg', '*.png', '*.ppm', '*.JPG']\n",
        "        self.all_image_paths = []\n",
        "\n",
        "        # Get all the image paths in sorted order.\n",
        "        for file_type in self.image_file_types:\n",
        "            self.all_image_paths.extend(glob.glob(os.path.join(self.dir_path, file_type)))\n",
        "        self.all_images = [image_path.split(os.path.sep)[-1] for image_path in self.all_image_paths]\n",
        "        self.all_images = sorted(self.all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Capture the image name and the full image path.\n",
        "        image_name = self.all_images[idx]\n",
        "        image_path = os.path.join(self.dir_path, image_name)\n",
        "\n",
        "        # Read and preprocess the image.\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image_resized = cv2.resize(image, (self.width, self.height))\n",
        "        image_resized /= 255.0\n",
        "\n",
        "        # Capture the corresponding XML file for getting the annotations.\n",
        "        annot_filename = os.path.splitext(image_name)[0] + '.xml'\n",
        "        annot_file_path = os.path.join(self.dir_path, annot_filename)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        tree = et.parse(annot_file_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Original image width and height.\n",
        "        image_width = image.shape[1]\n",
        "        image_height = image.shape[0]\n",
        "\n",
        "        # Box coordinates for xml files are extracted\n",
        "        # and corrected for image size given.\n",
        "        for member in root.findall('object'):\n",
        "            # Get label and map the `classes`.\n",
        "            labels.append(self.classes.index(member.find('name').text))\n",
        "\n",
        "            # Left corner x-coordinates.\n",
        "            xmin = int(member.find('bndbox').find('xmin').text)\n",
        "            # Right corner x-coordinates.\n",
        "            xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            # Left corner y-coordinates.\n",
        "            ymin = int(member.find('bndbox').find('ymin').text)\n",
        "            # Right corner y-coordinates.\n",
        "            ymax = int(member.find('bndbox').find('ymax').text)\n",
        "\n",
        "            # Resize the bounding boxes according\n",
        "            # to resized image `width`, `height`.\n",
        "            xmin_final = (xmin/image_width)*self.width\n",
        "            xmax_final = (xmax/image_width)*self.width\n",
        "            ymin_final = (ymin/image_height)*self.height\n",
        "            ymax_final = (ymax/image_height)*self.height\n",
        "\n",
        "            # Check that all coordinates are within the image.\n",
        "            if xmax_final > self.width:\n",
        "                xmax_final = self.width\n",
        "            if ymax_final > self.height:\n",
        "                ymax_final = self.height\n",
        "\n",
        "            boxes.append([xmin_final, ymin_final, xmax_final, ymax_final])\n",
        "\n",
        "        # Bounding box to tensor.\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # Area of the bounding boxes.\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 \\\n",
        "            else torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # No crowd instances.\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        # Labels to tensor.\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Prepare the final `target` dictionary.\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        image_id = torch.tensor([idx])\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "        # Apply the image transforms.\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image = image_resized,\n",
        "                                     bboxes = target['boxes'],\n",
        "                                     labels = labels)\n",
        "            image_resized = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "\n",
        "        if np.isnan((target['boxes']).numpy()).any() or target['boxes'].shape == torch.Size([0]):\n",
        "            target['boxes'] = torch.zeros((0, 4), dtype=torch.int64)\n",
        "        return image_resized, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "\n",
        "# Prepare the final datasets and data loaders.\n",
        "def create_train_dataset(DIR):\n",
        "    train_dataset = CustomDataset(\n",
        "        DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform()\n",
        "    )\n",
        "    return train_dataset\n",
        "def create_valid_dataset(DIR):\n",
        "    valid_dataset = CustomDataset(\n",
        "        DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform()\n",
        "    )\n",
        "    return valid_dataset\n",
        "def create_train_loader(train_dataset, num_workers=0):\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=False\n",
        "    )\n",
        "    return train_loader\n",
        "def create_valid_loader(valid_dataset, num_workers=0):\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=False\n",
        "    )\n",
        "    return valid_loader"
      ],
      "metadata": {
        "id": "GPdnVpjOLoW_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "gYSyqgnQL8-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.models.detection.ssd import (\n",
        "    SSD,\n",
        "    DefaultBoxGenerator,\n",
        "    SSDHead\n",
        ")\n",
        "\n",
        "def create_model(num_classes=91, size=300, nms=0.45):\n",
        "    model_backbone = torchvision.models.resnet34(\n",
        "        weights=torchvision.models.ResNet34_Weights.DEFAULT\n",
        "    )\n",
        "    conv1 = model_backbone.conv1\n",
        "    bn1 = model_backbone.bn1\n",
        "    relu = model_backbone.relu\n",
        "    max_pool = model_backbone.maxpool\n",
        "    layer1 = model_backbone.layer1\n",
        "    layer2 = model_backbone.layer2\n",
        "    layer3 = model_backbone.layer3\n",
        "    layer4 = model_backbone.layer4\n",
        "    backbone = nn.Sequential(\n",
        "        conv1, bn1, relu, max_pool,\n",
        "        layer1, layer2, layer3, layer4\n",
        "    )\n",
        "    out_channels = [512, 512, 512, 512, 512, 512]\n",
        "    anchor_generator = DefaultBoxGenerator(\n",
        "        [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        "    )\n",
        "    num_anchors = anchor_generator.num_anchors_per_location()\n",
        "    head = SSDHead(out_channels, num_anchors, num_classes)\n",
        "    model = SSD(\n",
        "        backbone=backbone,\n",
        "        num_classes=num_classes,\n",
        "        anchor_generator=anchor_generator,\n",
        "        size=(size, size),\n",
        "        head=head,\n",
        "        nms_thresh=nms\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "1mDIlG0yL-V0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1VMC7Q0wslgc"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Function for running training iterations.\n",
        "def train(train_data_loader, model):\n",
        "    print('Training')\n",
        "    model.train()\n",
        "\n",
        "     # initialize tqdm progress bar\n",
        "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
        "\n",
        "    for i, data in enumerate(prog_bar):\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = data\n",
        "\n",
        "        # move model parameters to DEVICE for doing computation\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "\n",
        "        train_loss_hist.send(loss_value)\n",
        "\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update the loss value beside the progress bar for each iteration\n",
        "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
        "    return loss_value\n",
        "\n",
        "# Function for running validation iterations.\n",
        "def validate(valid_data_loader, model):\n",
        "    print('Validating')\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize tqdm progress bar.\n",
        "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
        "    target = []\n",
        "    preds = []\n",
        "    for i, data in enumerate(prog_bar):\n",
        "        images, targets = data\n",
        "\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images, targets)\n",
        "\n",
        "        # For mAP calculation using Torchmetrics.\n",
        "        #####################################\n",
        "        for i in range(len(images)):\n",
        "            true_dict = dict()\n",
        "            preds_dict = dict()\n",
        "            true_dict['boxes'] = targets[i]['boxes'].detach().cpu()\n",
        "            true_dict['labels'] = targets[i]['labels'].detach().cpu()\n",
        "            preds_dict['boxes'] = outputs[i]['boxes'].detach().cpu()\n",
        "            preds_dict['scores'] = outputs[i]['scores'].detach().cpu()\n",
        "            preds_dict['labels'] = outputs[i]['labels'].detach().cpu()\n",
        "            preds.append(preds_dict)\n",
        "            target.append(true_dict)\n",
        "        #####################################\n",
        "\n",
        "    metric = MeanAveragePrecision()\n",
        "    metric.update(preds, target)\n",
        "    metric_summary = metric.compute()\n",
        "    return metric_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "1SlCKbuILEML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load train dataset from the directory\n",
        "train_dataset = create_train_dataset(TRAIN_DIR)\n",
        "\n",
        "# load validation dataset from the directory\n",
        "valid_dataset = create_valid_dataset(VALID_DIR)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
        "\n",
        "train_loader = create_train_loader(train_dataset, NUM_WORKERS)\n",
        "valid_loader = create_valid_loader(valid_dataset, NUM_WORKERS)\n",
        "\n",
        "\n",
        "# Initialize the model and move to the computation device.\n",
        "model = create_model(num_classes=NUM_CLASSES, size=RESIZE_TO)\n",
        "model = model.to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "# Total parameters and trainable parameters.\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params, lr=0.0005, momentum=0.9, nesterov=True\n",
        ")\n",
        "scheduler = MultiStepLR(\n",
        "    optimizer=optimizer, milestones=[45], gamma=0.1, verbose=True\n",
        ")\n",
        "\n",
        "# To monitor training loss\n",
        "train_loss_hist = Averager()\n",
        "# To store training loss and mAP values.\n",
        "train_loss_list = []\n",
        "map_50_list = []\n",
        "map_list = []\n",
        "\n",
        "# Mame to save the trained model with.\n",
        "MODEL_NAME = 'model'\n",
        "\n",
        "# Whether to show transformed images from data loader or not.\n",
        "if VISUALIZE_TRANSFORMED_IMAGES:\n",
        "    # from custom_utils import show_tranformed_image\n",
        "    show_tranformed_image(train_loader)\n",
        "\n",
        "# To save best model.\n",
        "save_best_model = SaveBestModel()\n",
        "\n",
        "# Training loop.\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
        "\n",
        "    # Reset the training loss histories for the current epoch.\n",
        "    train_loss_hist.reset()\n",
        "\n",
        "    # Start timer and carry out training and validation.\n",
        "    start = time.time()\n",
        "    train_loss = train(train_loader, model)\n",
        "    metric_summary = validate(valid_loader, model)\n",
        "    print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")\n",
        "    print(f\"Epoch #{epoch+1} mAP@0.50:0.95: {metric_summary['map']}\")\n",
        "    print(f\"Epoch #{epoch+1} mAP@0.50: {metric_summary['map_50']}\")\n",
        "    end = time.time()\n",
        "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    map_50_list.append(metric_summary['map_50'])\n",
        "    map_list.append(metric_summary['map'])\n",
        "\n",
        "    # save the best model till now.\n",
        "    save_best_model(\n",
        "        model, float(metric_summary['map']), epoch, 'outputs'\n",
        "    )\n",
        "    # Save the current epoch model.\n",
        "    save_model(epoch, model, optimizer)\n",
        "\n",
        "    # Save loss plot.\n",
        "    save_loss_plot(OUT_DIR, train_loss_list)\n",
        "\n",
        "    # Save mAP plot.\n",
        "    save_mAP(OUT_DIR, map_50_list, map_list)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eeb35fad37be4a218a399dc358d595ac",
            "bc575c556b3740f8ae2c7085f9ede50a",
            "34da890d24374b88a23a2b933304a303",
            "8af3d6eb9dc3415b8560d85c9e57364f",
            "768f641b599849039ffbe39be193b256",
            "63d717f78bb043eaac4318e8053569b0",
            "3acf732f05c2424c99cc310ed7afcb21",
            "653bd80e9f8342278e86c461862bd91a",
            "aafcf04e05c84a78b1e4f739266a4b91",
            "6f6c894f78d849ed8e330a33583e1504",
            "195d06bb5aa14f97af5f7fdb6c3074ea"
          ]
        },
        "id": "yckVR3DRJ2X8",
        "outputId": "26074d7b-bc02-46e2-a8f3-d0b3c11f2e13"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 1820\n",
            "Number of validation samples: 778\n",
            "\n",
            "SSD(\n",
            "  (backbone): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.15, 0.3, 0.44999999999999996, 0.6, 0.75, 0.9, 1.0], steps=None)\n",
            "  (head): SSDHead(\n",
            "    (classification_head): SSDClassificationHead(\n",
            "      (module_list): ModuleList(\n",
            "        (0): Conv2d(512, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1-3): 3 x Conv2d(512, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4-5): 2 x Conv2d(512, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (regression_head): SSDRegressionHead(\n",
            "      (module_list): ModuleList(\n",
            "        (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1-3): 3 x Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4-5): 2 x Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (transform): GeneralizedRCNNTransform(\n",
            "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "      Resize(min_size=(640,), max_size=640, mode='bilinear')\n",
            "  )\n",
            ")\n",
            "23,220,452 total parameters.\n",
            "23,220,452 training parameters.\n",
            "Adjusting learning rate of group 0 to 5.0000e-04.\n",
            "\n",
            "EPOCH 1 of 100\n",
            "Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/114 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eeb35fad37be4a218a399dc358d595ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-dc1983c80bfb>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Start timer and carry out training and validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mmetric_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-3f059c960551>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data_loader, model)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprog_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-33-108c8b8c6030>\", line 106, in __getitem__\n    sample = self.transforms(image = image_resized,\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 207, in __call__\n    p.preprocess(data)\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/utils.py\", line 83, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/utils.py\", line 91, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py\", line 142, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=True)\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py\", line 408, in convert_bboxes_to_albumentations\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py\", line 408, in <listcomp>\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py\", line 352, in convert_bbox_to_albumentations\n    check_bbox(bbox)\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py\", line 438, in check_bbox\n    raise ValueError(f\"x_max is less than or equal to x_min for bbox {bbox}.\")\nValueError: x_max is less than or equal to x_min for bbox (tensor(0.3060), tensor(0.5000), tensor(0.3060), tensor(0.5000), tensor(0)).\n"
          ]
        }
      ]
    }
  ]
}